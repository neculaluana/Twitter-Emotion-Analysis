{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neculaluana/Twitter-emotion-analysis/blob/main/emotion_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "### Overview\n",
        "Emotion analysis, a subfield of sentiment analysis, focuses on deciphering subjective information from text data to understand the underlying emotions. It plays a crucial role in various domains, including marketing, customer service, and social media monitoring, by providing insights into consumer behavior, public opinion, and personal sentiment. This project aims to explore the realm of emotion analysis by implementing machine learning techniques to classify text data into distinct emotion categories.\n",
        "\n",
        "### Objectives\n",
        "The primary objective of this project is to develop a robust emotion analysis model capable of accurately categorizing text into emotions such as happiness, sadness, anger, fear, love, and surprise. The project seeks to achieve the following:\n",
        "- **Understand the Distribution of Emotions**: Analyze the distribution of emotions in the dataset to gain insights into the predominant sentiments.\n",
        "- **Feature Engineering**: Identify and extract relevant features from the text data that significantly contribute to determining the emotion.\n",
        "- **Model Development and Validation**: Build and validate a model that can predict the emotion of a given text with high accuracy.\n",
        "\n",
        "### Dataset Description\n",
        "The dataset for this project consists of text data, each entry labeled with the corresponding emotion. The text data includes a diverse collection of sentences, phrases, and expressions, representing various contexts and scenarios.\n",
        "\n",
        "[Dataset Source](https://www.kaggle.com/datasets/parulpandey/emotion-dataset)\n",
        "\n",
        "### Approach\n",
        "The approach to tackling this problem involves a series of steps, starting from initial data exploration and preprocessing to model building, evaluation, and refinement. The project will clean and preprocess the text data to remove noise and standardize the format. Next, it will perform feature engineering to transform the text data into a format suitable for model input. Different machine learning models will be experimented with, their performance evaluated, and the best-performing model fine-tuned to enhance its predictive accuracy.\n",
        "\n",
        "### Expected Outcomes\n",
        "By the end of this project, it is expected to have a well-performing model that can accurately classify text into specific emotions. This model will provide valuable insights and serve as a tool for further applications in emotion analysis across various domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preliminary Setup\n",
        "This section handles the initial setup required for the emotion analysis project. It includes the installation and import of necessary libraries and modules, ensuring that all the tools needed for data handling, processing, and modeling are readily available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i8U9jQtfR7In"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import plotly.express as px\n",
        "\n",
        "from google.colab import drive\n",
        "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Loading and Preprocessing\n",
        "In this section, the dataset is loaded, and preliminary preprocessing is conducted. It involves removing unwanted characters, lower-casing, and other text normalization techniques to ensure consistency and improve the quality of the data for emotion analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('https://raw.githubusercontent.com/neculaluana/Twitter-emotion-analysis/main/input/training.csv?token=GHSAT0AAAAAACBUYC47QMRBYGX5L5L7H6FGZCBAPEA')\n",
        "df_test = pd.read_csv('https://raw.githubusercontent.com/neculaluana/Twitter-emotion-analysis/main/input/test.csv?token=GHSAT0AAAAAACBUYC477ZGWJ43J7OLYWPVGZCBAM7Q')\n",
        "df_valid = pd.read_csv('https://raw.githubusercontent.com/neculaluana/Twitter-emotion-analysis/main/input/validation.csv?token=GHSAT0AAAAAACBUYC47FCWVRUS45ZFJAQOYZCBAQDA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCjElxq2R_Uu",
        "outputId": "9c2393a9-d6ce-4185-e52b-86593133ad11"
      },
      "outputs": [],
      "source": [
        "def clean_tweet(tweet):\n",
        "\n",
        "  tweet = re.sub(r'https?://[^ ]+', '', str(tweet))      #removes links\n",
        "  tweet = re.sub(r'@[^ ]+', '', str(tweet))              #removes mentions\n",
        "  tweet = re.sub(r'#', '', str(tweet))                   #removes hashtag symbol\n",
        "  tweet = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', str(tweet)) #removes repeated characters ex: heeeeeeey\n",
        "  tweet = re.sub(r'[^A-Za-z ]', '', str(tweet))          #removes unwanted characters and punctuation\n",
        "  tweet = re.sub(r' 0 ', 'zero', str(tweet))             #transforms 0 to zero (it can influence emotion)\n",
        "  tweet = tweet.lower()                                  #lower-casing\n",
        "  return tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXnTyAaFe7yE"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_train, df_valid, df_test], ignore_index=True, sort=False)\n",
        "df[\"text\"]=df[\"text\"].apply(lambda text: clean_tweet(text))\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering and Dataset Preparation\n",
        "Here, custom features are defined, and the dataset is further structured and split into training, validation, and testing sets. This step is crucial for preparing the data in a format suitable for feeding into the emotion analysis model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "custom_features = Features({\n",
        "    'text': Value(dtype='string'),\n",
        "    'label': ClassLabel(names=emotion_names)\n",
        "})\n",
        "\n",
        "\"\"\"emotions_full = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(df_train,features=custom_features),\n",
        "    \"test\": Dataset.from_pandas(df_test,features=custom_features),\n",
        "    \"validation\": Dataset.from_pandas(df_valid,features=custom_features)\n",
        "    })\n",
        "\n",
        "\"\"\"\n",
        "emotions_full_dataset = Dataset.from_pandas(df, features=custom_features)\n",
        "data_column=emotions_full_dataset [\"text\"]\n",
        "label_column=emotions_full_dataset [\"label\"]\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(data_column, label_column, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_train, y_train, test_size=0.5, random_state=42)\n",
        "\n",
        "dataset = DatasetDict({\"train\": Dataset.from_dict({\"text\": X_train, \"label\": y_train}),\n",
        "                        \"validation\": Dataset.from_dict({\"text\": X_val, \"label\": y_val}),\n",
        "                        \"test\": Dataset.from_dict({\"text\": X_test, \"label\": y_test})})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Tokenization and Encoding\n",
        "In this section, the preprocessed text data is tokenized and encoded. This process converts the raw text into a format that can be fed into the model, typically involving converting words to numerical tokens and padding sequences to ensure consistent input sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding=True, truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize, batched=True, batch_size=None)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training and Validation\n",
        "This part of the notebook focuses on defining the emotion analysis model, setting up the training process, and training the model using the prepared dataset. It also involves validating the model's performance on the validation dataset to tune hyperparameters and ensure the model generalizes well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/gdrive/MyDrive/EmotionAnalysis/checkpoints60',          # output directory\n",
        "    num_train_epochs=3, # total number of training epochs\n",
        "    learning_rate=2e-5,             # learning rate\n",
        "    per_device_train_batch_size=64,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='/content/gdrive/MyDrive/EmotionAnalysis/logs60',            # directory for storing logs\n",
        "    logging_steps=10,             # log saving step\n",
        "    save_total_limit=1,             # number of total save model\n",
        "    load_best_model_at_end=True,    # load the best model when finished training (default metric is loss)\n",
        "    metric_for_best_model=\"accuracy\",   # use accuracy when comparing two models\n",
        "    greater_is_better=True,            # higher metric value is better\n",
        "    evaluation_strategy=\"epoch\",    # evaluate each `logging_steps`\n",
        "    save_strategy=\"epoch\",        # save each `logging_steps`\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model('/content/gdrive/MyDrive/EmotionAnalysis/models60')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_eval=trainer.evaluate(tokenized_datasets[\"test\"])\n",
        "print(result_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Testing and Evaluation\n",
        "The final section is dedicated to evaluating the trained model on the test dataset. It involves using the model to make predictions on unseen data and assessing its performance using appropriate metrics. The goal is to understand the model's effectiveness in accurately classifying emotions in text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"/content/gdrive/MyDrive/EmotionAnalysis/models60\", tokenizer=DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#prediction for one tweet\n",
        "emotion_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "predict_emotions = classifier(\"I can't believe someone would do such a thing\", return_all_scores=True)\n",
        "emotion_mapping = {\n",
        "    'LABEL_0': 'sadness',\n",
        "    'LABEL_1': 'joy',\n",
        "    'LABEL_2': 'love',\n",
        "    'LABEL_3': 'anger',\n",
        "    'LABEL_4': 'fear',\n",
        "    'LABEL_5': 'surprise'\n",
        "}\n",
        "#for prediction in predict_emotions:\n",
        "    #prediction[0] = emotion_mapping[prediction[\"label\"]]\n",
        "print(predict_emotions)\n",
        "\n",
        "df_preds = pd.DataFrame.from_records(predict_emotions[0])\n",
        "px.bar(x=emotion_names,y=100*df_preds['score'],template='plotly_white')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP6PyW90UkFOxzGyWsOqhnu",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
